{"nbformat":4,"nbformat_minor":2,"metadata":{"accelerator":"GPU","celltoolbar":"Tags","colab":{"name":"Week 9 Prac Neural Nets and Deep Learning.ipynb","provenance":[],"collapsed_sections":[]},"hide_input":false,"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","source":["# Practical Week 9: Neural Nets and Deep Learning\n","\n","In this practical you will implement simple convolutional neural nets (CNNs) to classify images. First, we will consider the well-known MNIST Handwriting Recognition example to familiarise ourselves with the Keras Tensorflow library that we will use. Then, you will implement a simple CNN that is capable of identifying what type of object is shown in an image.\n","\n","This work is not assessed, and you do not need to submit it. Please ask questions if you are facing difficulties with any of the content in this practical.\n","\n"],"metadata":{"id":"RRunrMQyJfUm"}},{"cell_type":"markdown","source":["For this practical, we may want to use a GPU. Training deep neural nets on a CPU will take a long time.\n","Google Colab provides us with some limited access to GPU-enabled kernels. There are some restrictions on uptime (~3 hours) and maximum GPU usage per day/week.\n","\n","To switch to a GPU-enabled kernel, go the *Runtime* menu at the top of the page, select *Change Runtime Type*, and select *GPU* under *Hardware accelerator*.\n","\n","Once you have done that, the kernel will restart.\n","Verify that the GPU is indeed available by executing this cell:"],"metadata":{"id":"AgnlKVPoNLBo"}},{"cell_type":"code","execution_count":null,"source":["!nvidia-smi"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2SZavOZON91z","executionInfo":{"status":"ok","timestamp":1633088466599,"user_tz":-570,"elapsed":455,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"39b487bb-1b5a-473e-ae94-d2dc2e948b9e"}},{"cell_type":"markdown","source":["If a GPU is available, we can see the GPU type (model name), GPU memory, and some further information about its state. Google Colab currently provides Tesla K80 and T4 GPUs. These are not the latest hardware, but will suffice for our purpose. After all, they are free to use!\n","\n","If you can't use Google Colab and don't have a GPU, you can still run the code but it will take significantly longer. What may take 2 minutes on a GPU can take more than 30 minutes on a fast CPU!\n","\n","Next, load the relevant libraries."],"metadata":{"id":"Q7bGgOc1OAaK"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tensorflow.keras.datasets import mnist, cifar10\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Conv2D, BatchNormalization, MaxPooling2D, Flatten\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from sklearn.metrics import classification_report\n"],"outputs":[],"metadata":{"id":"0MOppo-1KxDD","executionInfo":{"status":"ok","timestamp":1633088467810,"user_tz":-570,"elapsed":566,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"markdown","source":["## Activity 1: Load MNIST\n","\n","The MNIST dataset consists of 60,000 images of handwritten digits for training, and another 10,000 images for testing. Here are a few examples of the images:\n","\n","![MNIST Example Images](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n","\n","Our task is to create a neural net that can infer from the images which digit (0,...,9) is shown on each image.\n","\n","First, let's load the MNIST dataset and examine the data."],"metadata":{"id":"c81uoe72JWkd"}},{"cell_type":"code","execution_count":null,"source":["(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"outputs":[],"metadata":{"id":"gN2kW7G4L4VX","executionInfo":{"status":"ok","timestamp":1633088467811,"user_tz":-570,"elapsed":11,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"code","execution_count":null,"source":["X_train.shape, y_train.shape, X_test.shape, y_test.shape"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWOhV0vBL8LJ","executionInfo":{"status":"ok","timestamp":1633088467811,"user_tz":-570,"elapsed":11,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"d9aa340d-471f-41e5-f130-c3fa7d8a0283"}},{"cell_type":"markdown","source":["We learn that the training set consists of 60,000 square images of size 28x28, and that the test set contains 10,000 images of the same size. Each image is associated with the target label. \n","\n","Let's look at the first training image:"],"metadata":{"id":"3YThH4DgNSa6"}},{"cell_type":"code","execution_count":null,"source":["plt.imshow(X_train[0], cmap='gray');"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"6TcpIknxOdLE","executionInfo":{"status":"ok","timestamp":1633088468575,"user_tz":-570,"elapsed":770,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"2baf02bd-2cfe-4ae0-d957-3a95a8d43d80"}},{"cell_type":"markdown","source":["It appears to represent the digit '5'."],"metadata":{"id":"9qn_OfjnOnr5"}},{"cell_type":"markdown","source":["Let's verify that the correct label associated with the first training image is indeed '5':\n"],"metadata":{"id":"VFAc5QNRP13q"}},{"cell_type":"code","execution_count":null,"source":["y_train[0]"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bf4fN6KP8YQ","executionInfo":{"status":"ok","timestamp":1633088468576,"user_tz":-570,"elapsed":51,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"02353674-a53b-4d6c-f5ab-7ddf76235194"}},{"cell_type":"markdown","source":["Next, let's examine the distribution of the labels. You can use [`np.unique()`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) to determine the frequency of each label in the training vector (use `return_counts=True`).\n","\n","Is the data set balanced?"],"metadata":{"id":"w9KMveE1P94m"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["We see that each class appears about equally frequent, roughly 9-11% each, in the dataset. Thus, the dataset is balanced."],"metadata":{"id":"ii9s4UC4Q9fw"}},{"cell_type":"markdown","source":["Let's see how the image is represented in the data set:"],"metadata":{"id":"OvHXVv5RVwss"}},{"cell_type":"code","execution_count":null,"source":["X_train[0]"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ODRWZ79NlFn","executionInfo":{"status":"ok","timestamp":1633088468578,"user_tz":-570,"elapsed":44,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"f16d3879-7219-495b-8e15-c69d14d7a30d"}},{"cell_type":"markdown","source":["We have a 2-dimensional array of size 28x28, where the first dimension represents the rows and the second dimension represents the columns. \n","\n","We see that each entry in the image is a single value. That is, the image is a monochrome image (greyscale). Each value in the array reprsents a pixel intensity. In the image, maximum intensity (255) is shown as white colour, while zero is black.\n","\n","Let's see what the pixel value is at row 10 column 9. (The top left of the image is row 0 column 0.)"],"metadata":{"id":"wsa53SukO0LP"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Next, let's look at the value ranges in this image and in all images. Use [`min`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.min.html) and [`max`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.max.html)."],"metadata":{"id":"FKP51EYJRTBi"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["We see that the pixel values in the first training image range in 0,...,255. The same range is obained for the full data set. That is, there are no other images that have values outside this range."],"metadata":{"id":"ZDfMYURVRq98"}},{"cell_type":"markdown","source":["## Activity 2: Transform the Data\n","\n","To feed a 2-dimensional image into a neural net, we must first transform it into a 1-dimensional array. In this case, we must *flatten* the 28x28 array into an array of length 784. We can use [`np.reshape`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html) do perform this task for all images in the entire training set at once. Thus, the 60000x28x28 array becomes a 60000x784 array. We transform the test set in a similar way."],"metadata":{"id":"GOMsFlJrROIG"}},{"cell_type":"code","execution_count":null,"source":["X_train = X_train.reshape(60000,784)\n","X_test = X_test.reshape(10000,784)\n"],"outputs":[],"metadata":{"id":"mrbmj0F3WbA5","executionInfo":{"status":"ok","timestamp":1633088468580,"user_tz":-570,"elapsed":32,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"code","execution_count":null,"source":["X_train.shape, X_test.shape"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RPH3mf_mW0uQ","executionInfo":{"status":"ok","timestamp":1633088468581,"user_tz":-570,"elapsed":33,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"c5854ad3-2e0a-40e0-a20c-2884c192823a"}},{"cell_type":"markdown","source":["Next, we need to **normalize** the data. Neuronal nets work best when the input values are all small numbers in the range 0,...,1 or -1,...,1. Since the pixel intensities in the MNIST dataset range over 0,...,255, we must normalise them to 0,...,1. Do this by dividing each value in the training and test set by 255."],"metadata":{"id":"7i7m-PMMXCTV"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Verify that all values are in the range 0,...,1."],"metadata":{"id":"BdAcwkkqXuXu"}},{"cell_type":"code","execution_count":null,"source":["X_train.min(), X_train.max()"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9oxF9jAfXo5i","executionInfo":{"status":"ok","timestamp":1633088468583,"user_tz":-570,"elapsed":30,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"f8cff294-3a01-4948-b533-2aae21d4876d"}},{"cell_type":"markdown","source":["Next we look at the target labels associated with the images. Since there are 10 labels (digits '0',...,'9'), our neuronal net will have ten output units, one for each class. To train such a network, we must change the way the target label is represented. \n","\n","Instead of using a single variable that can take on ten different values, we will use ten different target variables which each can take on either zero or one. If a variable has value `1` then this means that the target label is the class associated with that variable. Since each image is associated with a unique class, at most one of these variables can have value 1, all other variables must be zero for each image.\n","\n","This encoding is called `one-hot encoding`, since at most one of the variables is non-zero for each image. Suppose that we introduce ten target variables, one for each digit. In this encoding, the first image (class 5) would be encoded as `[0,0,0,0,0,1,0,0,0,0]`. The variable representing class 5 is set to one while the other variables are set to zero.\n","\n","Next, we will change `y_train` and `y_test` to use one hot encoding. Function [`to_categorical`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) does this for us."],"metadata":{"id":"R2JgJ2m2X38f"}},{"cell_type":"code","execution_count":null,"source":["Y_train = to_categorical(y_train, 10)\n","Y_test = to_categorical(y_test, 10)"],"outputs":[],"metadata":{"id":"ATA1kWlJYypC","executionInfo":{"status":"ok","timestamp":1633088468584,"user_tz":-570,"elapsed":28,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"markdown","source":["We verify that the result has the correct shape and content:"],"metadata":{"id":"Js23SZS0ZdOl"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["We obtain the correct 60000x10 and 10000x10 shape. The first few rows of the 2-dimensional array look correct, too. They each have exactly one 1 in the position of the target class, and are zero elsewhere."],"metadata":{"id":"f1ehKXjnZjSi"}},{"cell_type":"markdown","source":["## Activity 3: Train a Fully Connected Feed Forward Network\n","\n","Now that we have prepared the data, we can train a fully connected feed forward neural net. \n","\n","We will create a simple network that consists of the following layers:\n","\n","* Hidden layer 1: 512 units, ReLU activation, followed by Dropout with probability 0.2\n","* Hidden layer 2: 512 units, ReLU activation, Dropout (0.2)\n","* Output layer: 10 units (one per class), softmax activation\n","\n","For each layer, we specify the number of units, the activation function. For units in the hidden layers we also adopt Dropout regularisation to avoid overfitting. \n","\n","Each unit in the layers is connected to each unit in the layer below (or the input if there is no lower layer). This architecture is called a *fully connected* (*\"dense\"*) network.\n","\n","The input to this network will be a vector of length 784 (one input per pixel in the input image).\n","\n","Let's create this network architecture using Keras: \n","\n","we create a [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) net architecture where we can add the layers one after another. We begin by adding the [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) connections between input and the first hidden layer. Here, we must specify the number of hidden units and the size of the input that the network will receive. Next, we add the [`relu`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation) activation function and [`Dropout`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) for the first hidden layer. We repeat this for the second layer, omitting the input size specification (as Keras infers this from the units in the layer below). Finally, we add the output layer units."],"metadata":{"id":"54Kd2gphaGG8"}},{"cell_type":"code","execution_count":null,"source":["def get_ff_mnist_model():\n","  model = Sequential()\n","  # Hidden layer 1\n","  # (784,) means a vector of length 784\n","  model.add(Dense(512, input_shape=(784,)))\n","  model.add(Activation('relu'))\n","  model.add(Dropout(0.2))\n","  # Hidden layer 2\n","  model.add(Dense(512))\n","  model.add(Activation('relu'))\n","  model.add(Dropout(0.2))\n","  # Output layer\n","  model.add(Dense(10))\n","  model.add(Activation('softmax'))\n","  return model\n","model = get_ff_mnist_model()"],"outputs":[],"metadata":{"id":"D-qls3ZZZxCq","executionInfo":{"status":"ok","timestamp":1633088469015,"user_tz":-570,"elapsed":14,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"markdown","source":["Let's print and verify the structure of the model."],"metadata":{"id":"o7RjThUKdQcQ"}},{"cell_type":"code","execution_count":null,"source":["model.summary()"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JGFQ-Bznb4xC","executionInfo":{"status":"ok","timestamp":1633088469016,"user_tz":-570,"elapsed":14,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"ef970b37-4f04-428d-897d-19e0e99ded3b"}},{"cell_type":"markdown","source":["We see that the elements we added are stacked on top of each other (the input enters at the top and the output is produced by the bottomost layer). For each layer, we wee the output shape that the layer produces, and the number of learnable weights (\"Param #\") that the model contains. We see that the net has 669,706 parameters that it must learn from the data.\n","\n","The `None` value in the first dimension on each layer represents the sample index in our data set. Since the model can be trained with any number of samples (we haven't told it how many there are), the value of this dimension is `None` (that means \"variable\")."],"metadata":{"id":"e5BLlgIudYfT"}},{"cell_type":"markdown","source":["Next, we must compile the model. Keras is built on top of Tensorflow, which is able to transform the model into code that can run directly on GPUs. This transformation is called model compilation. \n","\n","When compiling a model, we must specify a loss function, an optimiser, and one or more metrics. \n","\n","* The loss function we will use here is called *categorical cross-entropy*, and is a loss function that is well-suited to comparing two probability distributions. This is a good loss function for our classification problem, since we wish to minimise the differences between the distribution of `y_train` and the distribution of labels predicted by the model. Ideally, these two would be identical and the loss would be zero.\n","\n","* The optimiser helps to minimise the loss by gradient descent. It determines how to update the weights in the model to reduce the loss. This process is guided by a hyperparameter called *learning rate*. A larger learning rate may help the model learn faster, but it may not be able to learn the optimal weights if the learning rate is too large. We will use the popular *Adam* optimiser here, using its default learning rate.\n","\n","* The metrics measure how well the model is doing on the classification task. Here, we will use *Accuracy* as the metric to monitor. \n","\n","\n","Let's compile our model."],"metadata":{"id":"1BiFZPdBdPGz"}},{"cell_type":"code","execution_count":null,"source":["model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"outputs":[],"metadata":{"id":"br78zZNQeyNJ","executionInfo":{"status":"ok","timestamp":1633088469016,"user_tz":-570,"elapsed":11,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"markdown","source":["Now we are ready to train the model.\n","\n","Recall that neural nets are trained iteratively in *epochs*. In each epoch, the entire training data is fed into the model. Since even modern GPUs don't have sufficient memory to take all training data at once, we feed the data in batches. We must decide how many samples per batch size we shall use, and for how many epochs we may want to train our model.\n","\n","Here, we will use 128 samples per batch and train for 5 epochs. \n","\n","To be able to select a good model that does not suffer from overfitting, we hold out 20% of the 60,000 training samples. The optimiser will compute loss (\"val_loss\") and accuracy (\"val_accuracy\") on this held-out data set so that we can detect when the model begins to overfit. Recall that an increasing validation loss and decreasing validation accuracy are signs of overfitting.\n","\n","Keras provides a [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) function to train the model. It is similar to the fit function we have been using in scikit-learn in the past practicals. The fitting function returns a *history* object that we can interrogate for information about each epoch. "],"metadata":{"id":"HKaGPZUlfhsD"}},{"cell_type":"code","execution_count":null,"source":["history = model.fit(X_train, Y_train, batch_size=128, epochs=5, validation_split=0.2, verbose=1)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5BPvcQ4QgbxR","executionInfo":{"status":"ok","timestamp":1633088479681,"user_tz":-570,"elapsed":10675,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"36c30fbd-4d69-4c44-c57c-3c2cf0bb616c"}},{"cell_type":"markdown","source":["Training this model on a CPU takes about 7 second per epoch, for at total of ~34 seconds. On a GPU, we can achieve the same in about 2 seconds per epoch.\n","\n","We wee that the optimiser has reduced the loss from ~0.28 to ~0.047 while accuracy has increased from ~0.92 to ~0.98 on the trainig set as training progressed. At the same time, accuracy on the validation set increased from 0.96 to 0.975 after epoch 3, then started to decline slightly. This tells us that the model obtained at the end of epoch 3 is perhaps the one we shall select. (Your results may differ slightly.)\n","\n","Let's plot the learning curve to confirm:"],"metadata":{"id":"bJ9XveUYhdFM"}},{"cell_type":"code","execution_count":null,"source":["training_df = pd.DataFrame.from_dict(history.history).assign(epoch=np.array(history.epoch)+1)\n","sns.lineplot(data=training_df, x='epoch', y='accuracy', color='green');\n","sns.lineplot(data=training_df, x='epoch', y='val_accuracy', color='red');"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"wb8hRs2thUVT","executionInfo":{"status":"ok","timestamp":1633088480401,"user_tz":-570,"elapsed":732,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"4bcb5495-ddf0-48c4-d433-0d55d84eecfd"}},{"cell_type":"markdown","source":["We confirm that the model at epoch 3 yields best validation accuracy.\n","\n","Now we can re-create the best model by stopping training after 3 epochs. We re-create the model, compile, and train. However this time we don't need the validation split as we have already chosen the best model."],"metadata":{"id":"SK4V5BLekzVc"}},{"cell_type":"code","execution_count":null,"source":["model = get_ff_mnist_model()\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X_train, Y_train, batch_size=128, epochs=3, verbose=1)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLMODbtSjs8D","executionInfo":{"status":"ok","timestamp":1633088486691,"user_tz":-570,"elapsed":6311,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"bd0c17ee-5e59-414d-cb49-5d1f8cd1118d"}},{"cell_type":"markdown","source":["## Activity 4: Evaluate the model\n","\n","Now that we have trained our model, let's test how well it generalises to unseen data."],"metadata":{"id":"EQLlp_Eim3lc"}},{"cell_type":"code","execution_count":null,"source":["model.evaluate(X_test, Y_test)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sNzFvlX9ms4r","executionInfo":{"status":"ok","timestamp":1633088488202,"user_tz":-570,"elapsed":1532,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"e0b98999-4886-49ac-b34d-4aa4c60faa22"}},{"cell_type":"markdown","source":["We see that the accuracy on the test set closely matches the accuracy on the validation set during training. This builds our confidence that the model has learned well.\n","\n","To analyse its performance in mode detail, let's obtain predictions for the each of the test set images:"],"metadata":{"id":"mm6RW2lxnMtm"}},{"cell_type":"code","execution_count":null,"source":["Y_pred_proba = model.predict(X_test)"],"outputs":[],"metadata":{"id":"D1ZvJNnunIW7","executionInfo":{"status":"ok","timestamp":1633088488910,"user_tz":-570,"elapsed":714,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"code","execution_count":null,"source":["Y_pred_proba.shape"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VaS292lWncOL","executionInfo":{"status":"ok","timestamp":1633088488910,"user_tz":-570,"elapsed":28,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"156dd12b-620c-45f9-c07a-722e519aac9d"}},{"cell_type":"code","execution_count":null,"source":["Y_pred_proba[:10].round(2)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tgwILyMvneFz","executionInfo":{"status":"ok","timestamp":1633088488911,"user_tz":-570,"elapsed":16,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"c250b54f-e8b4-47ad-f129-c2d786e883a8"}},{"cell_type":"markdown","source":["For each image, we obtain 10 values, one for each class. The largest value denotes the class that the model predicts. If there is a single largest value then the model is \"sure\" of its prediction (but it may be sure yet be wrong!); otherwise if there are multiple values that are close to being largest then the model does not yield a clear unique prediction. After rounding to two decimal places, we see that for many images there is a single class predicted (a single 1 in each row). For some images, multiple non-zero values exist, but one dominates the others. This means that there is a good margin between the prediction of the most likely class and the next less-likely class. Although the classifier predicts a single most likely class for the samples we have inspected, this does not tell us if that prediction is actually correct. We need to compare this to the ground truth in the test set.\n","\n","Let's find the predicted class (the index where the predicted probability is largest) in each row, and compute a classification report."],"metadata":{"id":"ch4vMHe_nk-d"}},{"cell_type":"code","execution_count":null,"source":["y_pred = np.argmax(Y_pred_proba, axis=1)"],"outputs":[],"metadata":{"id":"0yB4cvyUoSyC","executionInfo":{"status":"ok","timestamp":1633088488913,"user_tz":-570,"elapsed":14,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["We observe that all metrics are close to one. Our classifier works well. :-)"],"metadata":{"id":"l0QWjizAo-Kh"}},{"cell_type":"markdown","source":["# Activity 5: Train a CNN\n","\n","We will repeat the above process with a Convolutional neural network architecture.\n","\n","We begin by preparing the data in the same way as before, except that\n","* we do not flatten the images into a 1-dimensional array, and\n","* we add an additional dimension at the end (for channel).\n","\n","We keep its two dimensions since the CNN will apply convolution operations to the 2-dimensional structure, and we add the additional dimension since the CNN architecture assumes that there is a channel dimension present, even if there is only once channel in a monochrome image.\n"],"metadata":{"id":"m0FIWhpZqSSo"}},{"cell_type":"code","execution_count":null,"source":["(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"outputs":[],"metadata":{"id":"qkS3trK8q6x5","executionInfo":{"status":"ok","timestamp":1633088489407,"user_tz":-570,"elapsed":502,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"code","execution_count":null,"source":["# add an additional dimension to represent the single-channel\n","X_train = X_train.reshape(60000, 28, 28, 1)\n","X_test = X_test.reshape(10000, 28, 28, 1)\n","X_train = X_train / 255\n","X_test = X_test / 255"],"outputs":[],"metadata":{"id":"PSOxjvhUq7Ms","executionInfo":{"status":"ok","timestamp":1633088489408,"user_tz":-570,"elapsed":33,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"markdown","source":["We'll verify shape and contents of the first image:"],"metadata":{"id":"tKGzCnq02TJX"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["We see that the additional dimension (1 at the end of the shape vector) has been added, and that each value in the image array is now a singleton array instead of a plain floating point number.\n","\n","We can use one hot encoding for the target labels in the training and the testing set as before."],"metadata":{"id":"fe-aBBNz2bLi"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Next, we create a CNN.\n","\n","We'll use a simple architecture with 4 layers of convolutions, followed by 2 fully connected layers. Each convolution block begins with convolution filters of size 3x3. The number of filters varies in different layers; the first two layers use 32 [`Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) filters whereas the subsequente two layers use 64 filters. After the convolution, [`BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) is applied. Batch normalization is a technique to rescale the activations to improve training performance. After normalization, the [`relu Activation`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation) is applied, followed by 2x2 [`MaxPooling`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D). This process repeats 4 times. At the end, we obtain 64 4x4 feature maps as the output of the last MaxPooling operation. These 64 matrices are then flattened into a 1-dimensional vector of length 1024. The remaining layers add a fully connected [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) network on top of that vector. [`BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) is used to improve the training, and [`Dropout`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) is applied to combat overfitting. \n"],"metadata":{"id":"VzQjjDSv3BsC"}},{"cell_type":"code","execution_count":null,"source":["\n","def get_mnist_cnn_model():\n","  model = Sequential()\n","\n","  # Convolution Layer 1: 32 feature maps\n","  model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu', name='layer1'))\n","\n","  # Convolution Layer 2: 32 feature maps\n","  model.add(Conv2D(32, (3, 3)))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(MaxPooling2D(pool_size=(2,2), name='layer2'))\n","\n","  # Convolution Layer 3: 64 feature maps\n","  model.add(Conv2D(64,(3, 3)))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu', name='layer3'))\n","\n","  # Convolution Layer 4: 64 feature maps\n","  model.add(Conv2D(64, (3, 3)))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(MaxPooling2D(pool_size=(2,2), name='layer4'))\n","  model.add(Flatten())\n","\n","  # Fully Connected Layer 5\n","  model.add(Dense(512))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","\n","  # Fully Connected Layer 6                       \n","  model.add(Dropout(0.2))\n","  model.add(Dense(10))\n","  model.add(Activation('softmax'))\n","\n","  return model\n","model = get_mnist_cnn_model()"],"outputs":[],"metadata":{"id":"omjS7K9y3AQ6","executionInfo":{"status":"ok","timestamp":1633088489868,"user_tz":-570,"elapsed":466,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"markdown","source":["Inspect the model structure."],"metadata":{"id":"lhHQzfygWmHb"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["We can see that the layers progressively reduce the size of the image (from 28x28 down to 4x4) while doubling the number of channels (feature maps) in each layer. Eventually, the 64 feature maps of size 4x4 are flattened into a 1-dimensional vector of length 1024, which is then fed into two layers of fully connected layers that compute the classification output.\n","\n","Let's compile and train the model as before. It will take ~ 1 minute when using a GPU."],"metadata":{"id":"QwLa8FwmWK2L"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Plot the accuracy on the training set and the validation set, and determine the optimal number of epochs."],"metadata":{"id":"1mls8K_EHVwJ"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["We conclude that the validation result has stabilised after 2 (or 3) epochs. We now re-train the model on the full training set, stopping after 2 epochs."],"metadata":{"id":"AD3_kofIHemq"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["After ~22 seconds the model has completed training (when using a GPU).\n","\n","## Activity 6: Evaluate the CNN Model\n","\n","Let's evalute the model on the test set. Use the same procedure as for the fully connected network we trained earlier."],"metadata":{"id":"jtejs3AYHskU"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["We find that all metrics are very close to 1. The model works even better than the fully connected network we trained earlier!\n","\n","However, it is not perfect. Let's see where the CNN errs. Let's see on which images the model produces results that differ from the ground truth in the test set."],"metadata":{"id":"vwrU8l8TH_a4"}},{"cell_type":"code","execution_count":null,"source":["# images it gets wrong\n","wrong_idx = np.nonzero(y_pred != y_test)[0]\n","len(wrong_idx)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VzF69p3YnFq","executionInfo":{"status":"ok","timestamp":1633088594427,"user_tz":-570,"elapsed":18,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"2c7dd92b-01ef-4f06-ae06-f2c520f67167"}},{"cell_type":"markdown","source":["We find that there are 116 images where predictions are wrong. (Your numbers may differ.) Let's visualise some of them."],"metadata":{"id":"puMOXPZOIUiU"}},{"cell_type":"code","execution_count":null,"source":["def show_image(idx):\n","  wrong_img_data = X_test[idx].reshape(28,28)\n","  print(f'Image {idx} predicted as {y_pred[idx]} but is actually {y_test[idx]}.')\n","  plt.imshow(wrong_img_data, cmap='gray')\n","for idx in wrong_idx[:5]:\n","  show_image(idx)\n","  plt.show()"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"rnSaIH7MaD8p","executionInfo":{"status":"ok","timestamp":1633088595117,"user_tz":-570,"elapsed":701,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"5644c489-2674-4f31-8fb7-9198959ef3b1"}},{"cell_type":"markdown","source":["Some of them are actually quite difficult to decipher."],"metadata":{"id":"2hhh5U9xI5Hp"}},{"cell_type":"markdown","source":["Finally, we can peek inside the CNN and visualise how the network sees an image. Let's pick the first training image (a '5') and visualise the feature maps that the model generates at the output of each of the 4 convolution blocks in the model. (The corresponding layers are labelled 'layer1'...'layer4' in the model.)"],"metadata":{"id":"YhEymQszJIjc"}},{"cell_type":"code","execution_count":null,"source":["from tensorflow.keras.models import Model"],"outputs":[],"metadata":{"id":"5sxyPsUDaK8S","executionInfo":{"status":"ok","timestamp":1633088595118,"user_tz":-570,"elapsed":5,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"code","execution_count":null,"source":["def visualise_features(model, layers, image):\n","  outputs = [model.get_layer(layer).output for layer in layers]\n","  vis_model = Model(inputs=model.inputs, outputs=outputs)\n","  image_data = np.expand_dims(image, axis=0)\n","  feature_maps = vis_model.predict(image_data)\n","  for fmap in feature_maps:\n","    channels = fmap.shape[-1]\n","    square = int(np.ceil(np.sqrt(channels)))\n","    for ix in range(channels):\n","      # specify subplot and turn of axis\n","      ax = plt.subplot(square, square, ix+1)\n","      ax.set_xticks([])\n","      ax.set_yticks([])\n","      # plot filter channel in grayscale\n","      plt.imshow(fmap[:, :, ix], cmap='gray')"],"outputs":[],"metadata":{"id":"e5aGFciGcat7","executionInfo":{"status":"ok","timestamp":1633088595119,"user_tz":-570,"elapsed":6,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"code","execution_count":null,"source":["for layer in ['layer1','layer2','layer3','layer4']:\n","  print(f'Layer {layer}:')\n","  visualise_features(model, [layer], X_train[0] )\n","  plt.show()"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UqRjV2Y4d0EZ","executionInfo":{"status":"ok","timestamp":1633088604002,"user_tz":-570,"elapsed":8888,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"b126ed73-53f9-4a4a-da95-1e98521470f0"}},{"cell_type":"markdown","source":["## Activity 8: Train a CNN for Image Classification\n","\n","Now that we have successfully built a CNN for handwritten digits, let's expand our work to image classification. You will create a CNN classifier for images across a wide range of different objects based on the CIFAR10 data set.\n","\n","You will follow the same process as before: understand and prepare the data, define and train a model, and evaluate the model on the test set. \n","\n","Let's begin by loading the CIFAR10 dataset. The dataset consists of 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them."],"metadata":{"id":"VvY3eRMye51T"}},{"cell_type":"code","execution_count":null,"source":["(X_train, y_train), (X_test, y_test) = cifar10.load_data()"],"outputs":[],"metadata":{"id":"WrZ_gJLdNmcc","executionInfo":{"status":"ok","timestamp":1633088604643,"user_tz":-570,"elapsed":656,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"code","execution_count":null,"source":["X_train.shape, X_test.shape"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghQsqURjN6p2","executionInfo":{"status":"ok","timestamp":1633088604644,"user_tz":-570,"elapsed":18,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"c6cfd2c5-fee6-4fd4-fd52-6579a7b7d466"}},{"cell_type":"code","execution_count":null,"source":["y_train.shape, y_test.shape"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbMEcERdXRNE","executionInfo":{"status":"ok","timestamp":1633088604644,"user_tz":-570,"elapsed":8,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"4b7056e1-3485-4988-d4be-0ab1ca0101e4"}},{"cell_type":"code","execution_count":null,"source":["# The CIFAR labels happen to be arrays. Reshape the arrays to obtain 1-dimensional vectors.\n","y_train = y_train.reshape((-1,))\n","y_test = y_test.reshape((-1,))"],"outputs":[],"metadata":{"id":"ZtG5kfU4W9-y","executionInfo":{"status":"ok","timestamp":1633088604645,"user_tz":-570,"elapsed":6,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"markdown","source":["We see that there are 60,000 images in total. Each image is 32x32 in size and has three channels (RGB).\n","\n","As for MNIST, we must normalize the images to the range [0,..,1] and one-hot encode the target class labels."],"metadata":{"id":"6G5_dtbDN_qU"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Let's plot a few images to see what is in this data set."],"metadata":{"id":"aOlWTbKVOUqT"}},{"cell_type":"code","execution_count":null,"source":["class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']\n","\n","plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(X_train[i])\n","    plt.xlabel(class_names[y_train[i]])"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"id":"PzjUgazhORYk","executionInfo":{"status":"ok","timestamp":1633088608926,"user_tz":-570,"elapsed":3847,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"f0659c36-6fa5-4b93-f2fa-3ba94f007918"}},{"cell_type":"markdown","source":["Although the images are a little blurry, we can make out what is on each of them reasonably easily.\n","\n","Let's see if we can adapt the CNN we used earlier to this new data set.\n","\n","We will use the same layers, but change the input dimensions to 32x32x3 to match the CIFAR10 images.\n"],"metadata":{"id":"ZpZzGADLOl-m"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Summarise the model to see its structure:"],"metadata":{"id":"p_hiEKxAPUnK"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Then, compile and train the model.\n","\n","We follow the same steps as for MNIST but train for 10 epochs and use a validation split of 10%. This will take ~3 minutes on a GPU."],"metadata":{"id":"caIQU8gyPbPb"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Plot the learning curve determine the number of epochs that yield the model that performs best on the validation split."],"metadata":{"id":"QtVa9KlLGAex"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["We wee that the validation results climb until the end of epoch 6, and then decline and fluctuate. Although the model at epoch 10 has slightly higher validation accuracy, the gap between the training and the validation split has become large (which you may recall that this is a sign that the model has overfitted). Hence, we will select the model at 6 epochs. (Your results may differ.)\n","\n","Let's train the model on the full training set for 6 epochs. This may take ~2 minutes on a GPU."],"metadata":{"id":"ysisoqvhGJzj"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Now, test how well the model performs on the test set. Construct a classification report to assess the model's results for each class."],"metadata":{"id":"EuLT3olaG42v"}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Looking at the F1 scores, we learn that the model performs reasonably well for classes automobile, frog, horse, ship, and truck. It performed worst overall for cat and dog. \n","\n","Construct a confusion matrix to see how the model errs. Use the function below to plot the matrix, since plugging the Keras classifier into the scikit-learn function plot_confusion_matrix() will not work."],"metadata":{"id":"AuVE-mBgHRbL"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.metrics import confusion_matrix\n","def plot_cm(y_true, y_pred, class_names):\n","  cm = confusion_matrix(y_true, y_pred)\n","  cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n","  sns.heatmap(cm_df, annot=True, fmt='d'); # rows are true labels, columns are predictions"],"outputs":[],"metadata":{"id":"0jpZ8C5mVwSl","executionInfo":{"status":"ok","timestamp":1633088901983,"user_tz":-570,"elapsed":7,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}}}},{"cell_type":"code","execution_count":null,"source":["# TODO"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["We see that cats are often mistaken as deer or frog, and dogs are often mistaken as cats.\n","\n","Let's look at some of the images that the model gets wrong."],"metadata":{"id":"gSsdj451Isee"}},{"cell_type":"code","execution_count":null,"source":["wrong_idx = np.nonzero(y_pred != y_test)[0]\n","print(len(wrong_idx),' images classified incorrectly')\n","\n","def show_image(idx):\n","  print(f'Image {idx} predicted as {class_names[y_pred[idx]]} but is actually {class_names[y_test[idx]]}.')\n","  plt.imshow(X_test[idx], cmap='gray')\n","for idx in wrong_idx[:5]:\n","  show_image(idx)\n","  plt.show()"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fdLxRuEuVIqN","executionInfo":{"status":"ok","timestamp":1633089560408,"user_tz":-570,"elapsed":1308,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"55472ca7-c077-4fac-dc24-6c52fc573ca3"}},{"cell_type":"markdown","source":["We can also look at some of the images that the model predicted correctly."],"metadata":{"id":"N4dPBntTJg3d"}},{"cell_type":"code","execution_count":null,"source":["correct_idx = np.nonzero(y_pred == y_test)[0]\n","print(len(correct_idx),' images classified correctly')\n","\n","def show_image(idx):\n","  print(f'Image {idx} correctly predicted as {class_names[y_pred[idx]]}.')\n","  plt.imshow(X_test[idx], cmap='gray')\n","for idx in correct_idx[:5]:\n","  show_image(idx)\n","  plt.show()"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"TELwEG-GgFhv","executionInfo":{"status":"ok","timestamp":1633089578196,"user_tz":-570,"elapsed":1777,"user":{"displayName":"Wolfgang Mayer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18418719765368578413"}},"outputId":"7c5e5512-4634-4ac3-c1f3-8426c9956032"}},{"cell_type":"markdown","source":["This concludes our investigation into CNNs for CIFAR10. The model we have designed here served us to learn how to construct, train, and analyse a model. The model is however still far from the best possible model that can be constructed. It is possible to design improved models that exhibit >96% accuracy on CIFAR10, and ~99.8% on MNIST. You can see the [list of high scores](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130). To achieve this, more advanced models and data augmentation techniques must be used. If you are interested, [read about further ways to improve CNN models for image classification](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/). \n","\n","This concludes the practical. You should now be able to define a given CNN architecture in Keras and train and evaluate the model."],"metadata":{"id":"YzDs5ufIJe4B"}}]}